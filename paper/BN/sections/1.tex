\documentclass[../main.tex]{subfile}
\graphicspath{{\subfix{../images}}}
\begin{document}

深度学习极大地推动了视觉、语音和其他领域的发展。随机梯度下降（Stochastic gradient descent, SGD），已经被证明为是一种训练深度网络的有效方式。为了达到最好的性能，例如momentum\cite{momentum}和Adagrad\cite{adagrad}的SGD变种也会被使用。SGD会优化网络的参数$\Theta$来最小化损失
\begin{equation*}
    \Theta = \arg\min_{\Theta} \frac{1}{N}\sum_{i=1}^{N}\ell(\text{x}_i, \Theta)
\end{equation*}
其中$\text{x}_{1\ldots N}$是训练数据集。使用SGD，训练分步进行，在每一步中我们考虑大小为$m$的\textit{迷你批}$\text{x}_{1\ldots m}$。我们使用通过迷你批计算的
\begin{equation*}
    \frac{1}{m} \frac{\partial \ell(\text{x}_i, \Theta)}{\partial \Theta}
\end{equation*}
来近似损失函数对于参数的梯度。使用多个样例的迷你批，与每次仅使用一个样例相比，在以下几个方面会有帮助。首先，损失函数在迷你批的梯度是损失函数在训练集的梯度的估计，它的质量会随着批大小的增加而提高。其次，由于现代计算平台提供的并行，在一个批上的计算会比为单独的样例做$m$次计算更加高效。

虽然随机梯度是简单且有效的，但是它要求对模型超参数的精心调整，尤其是在优化中使用的学习率和模型参数的初始值。由于每一层的输入都会被前面所有层的参数影响，所以网络参数的微小变换会随着网络变深而被放大，这使训练变得复杂。

改变某一层的输入分布会使得这一层需要持续适应新的分布，这实际上是一个问题。当一个学习系统的输入分布变换时，我们称它经历了\textit{协变量偏移}\cite{covariateshift}。这通常通过领域适应处理。然而，协变量偏移的概念可以扩展到整个学习系统之外，应用于其部分，例如子网络或某一层。考虑一个计算
\begin{equation*}
    \ell = F_2(F_1(\text{u}, \Theta_1), \Theta_2)
\end{equation*}
的网络，其中$\text{F}_1$和$\text{F}_2$是任意的变换，通过优化参数$\Theta_1,\Theta_2$来最小化损失$\ell$。学习$\Theta_2$可以被视为输入$\text{x}=\text{F}_1(\text{u}, \Theta_1)$被送入子网络
\begin{equation*}
    \ell = \text{F}_2(\text{x}, \Theta_2)
\end{equation*}
例如，一步梯度下降（批大小为$m$，学习率为$\alpha$）
\begin{equation*}
    \Theta_2 \leftarrow \Theta_2 - \frac{\alpha}{m}\sum_{i=1}^{m}\frac{\partial \text{F}_2\left(\text{x}_i, \Theta_2\right)}{\Theta_2}
\end{equation*}
与为单独的$\text{F}_2$网络输入$\text{x}$是完全等价的。因此，使得训练更加高效的输入分布性质——例如训练数据和测试数据有相同的分布——对于训练子网络也是适用的。因此，随着时间的推移，保持$\text{x}$的分布固定是有利的。此后，$\Theta_2$不必为弥补$\text{x}$分布变换而从新调整。

固定子网络输入分布对于子网络之外的层也有积极影响。考虑某一层使用sigmoid$\text{z} = g(W\text{u} + \text{b})$作为激活函数，其中$\text{u}$是这一层的输入，参数矩阵$W$和偏差向量$\text{b}$是这一层将要学习的参数，$g(x) = \frac{1}{1 + \exp(-x)}$。随着$|x|$变大，$g^\prime (x)$会趋向于0。这意味这对于$\text{x}=W\text{u} + \text{b}$来说，除绝对值较小的维度之外的所有维度，\text{u}的梯度将会消失，进而导致模型训练缓慢。然而，由于x会被$W$，b和之前所有层的参数影响，所以在训练中改变这些参数将会很可能将x的很多维度移动到非线性的饱和区并减缓收敛。这个效应会随着网络深度的提高而被放大。在实际中，饱和问题以及它所导致的梯度消失通常通过使用修正线性单元（Rectified Linear Units）\cite{relu}$\text{ReLU}(x)=\max(x, 0)$，精心初始化和小学习率来解决。然而，如果我们可以在网络训练时保证非线性的输入分布保持稳定，那么优化器就更可能不被困在饱和区，进而训练也会更快。

我们将训练过程中深度网络内部节点分布的变换成为\textit{协变量偏移}。消除它将会使得训练更快。我们提出了一种名为批归一化的机制，它朝减少内部协变量偏移迈出了一步，这样做显著加速了深度神经网络的训练。它通过归一化步骤来实现，归一化将会固定每一层输入的均值和方差。批归一化通过减少梯度对于参数尺度或者它们的初始值的依赖，对于梯度在网络流动也有好处。这使得我们可以使用更高的学习率，同时不会导致发散。不仅如此，批归一化减少了模型过拟合的风险并减少了对于Dropout\cite{dropout}的需要。最后，批归一化通过防止网络被困在饱和区，使得使用饱和的非线性成为可能。

在4.2节中，我们为性能最好的ImageNet分类网络应用了批归一化，并显示了我们可以仅使用7\%的训练步骤来达到它原有的性能，并最终显著超过了它原有的准确率。使用一个集成了以批归一化训练的这样的模型，我们达到了在ImageNet分类任务上的已知最好top-5错误率。

\end{document}